{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"siddharth_day_2.ipynb","provenance":[],"authorship_tag":"ABX9TyNA2XhRonn2E9ZuLpgCKQjs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4V8cKjJiCb_U","executionInfo":{"status":"ok","timestamp":1627407043848,"user_tz":-330,"elapsed":40671,"user":{"displayName":"Siddharth Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzyE_nM4YvSHuRfgimq1KB8lKCYvlaadB3eT23qw=s64","userId":"11688753670621557151"}},"outputId":"9f520868-1f33-47c7-ea81-dabed4a33632"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive') "],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WS4K41bU_4Ds","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627407091349,"user_tz":-330,"elapsed":524,"user":{"displayName":"Siddharth Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzyE_nM4YvSHuRfgimq1KB8lKCYvlaadB3eT23qw=s64","userId":"11688753670621557151"}},"outputId":"e0905f22-90c6-4e62-9086-69c23a50f532"},"source":["%cd /content/gdrive/MyDrive/MSc_CL/Machine Learning/lxmls-toolkit"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/MSc_CL/Machine Learning/lxmls-toolkit\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RONC21FuDIyJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627407094437,"user_tz":-330,"elapsed":540,"user":{"displayName":"Siddharth Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzyE_nM4YvSHuRfgimq1KB8lKCYvlaadB3eT23qw=s64","userId":"11688753670621557151"}},"outputId":"1d413579-b1a4-4486-b9bf-ec9bfa1f8df0"},"source":["!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["data\t     lxmls\t  README.md\t    siddharth_day_0.ipynb  tests\n","labs\t     MANIFEST.in  requirements.txt  siddharth_day_1.ipynb  tox.ini\n","LICENSE.txt  __pycache__  setup.py\t    siddharth_day_2.ipynb\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qw6kf4ay_j0k"},"source":["## Exercise 2.1 \n","To ease-up the upcoming implementation exercise, examine and comment the following implementation of\n","a log-linear model and its gradient update rule. Start by loading Amazon sentiment corpus used in day 1"]},{"cell_type":"code","metadata":{"id":"uUyiQg10_hlw","executionInfo":{"status":"ok","timestamp":1627407287891,"user_tz":-330,"elapsed":4755,"user":{"displayName":"Siddharth Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzyE_nM4YvSHuRfgimq1KB8lKCYvlaadB3eT23qw=s64","userId":"11688753670621557151"}}},"source":["import lxmls.readers.sentiment_reader as srs\n","from lxmls.deep_learning.utils import AmazonData\n","corpus=srs.SentimentCorpus(\"books\")\n","data = AmazonData(corpus=corpus)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DjmJhUMKIBge"},"source":["Compare the following numpy implementation of a log-linear model with the derivations seen in the previous sections.\n","Introduce comments on the blocks marked with # relating them to the corresponding algorithm steps."]},{"cell_type":"code","metadata":{"id":"_tADA9m2ICqj","executionInfo":{"status":"ok","timestamp":1627408067208,"user_tz":-330,"elapsed":412,"user":{"displayName":"Siddharth Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzyE_nM4YvSHuRfgimq1KB8lKCYvlaadB3eT23qw=s64","userId":"11688753670621557151"}}},"source":["from lxmls.deep_learning.utils import Model, glorot_weight_init, index2onehot, logsumexp\n","import numpy as np\n","\n","class NumpyLogLinear(Model):\n","\n","  def __init__(self, **config):\n","\n","    # Initialize parameters\n","    weight_shape = (config['input_size'], config['num_classes'])\n","    # after Xavier Glorot et al\n","    self.weight = glorot_weight_init(weight_shape, 'softmax')\n","    self.bias = np.zeros((1, config['num_classes']))\n","    self.learning_rate = config['learning_rate']\n","\n","  def log_forward(self, input=None):\n","    \"\"\"Forward pass of the computation graph\"\"\"\n","\n","    # Linear transformation\n","    z = np.dot(input, self.weight.T) + self.bias\n","\n","    # Softmax implemented in log domain\n","    log_tilde_z = z - logsumexp(z, axis=1, keepdims=True)\n","\n","    return log_tilde_z\n","\n","  def predict(self, input=None):\n","    \"\"\"Prediction most probable class index\"\"\"\n","    return np.argmax(np.exp(self.log_forward(input)), axis=1)\n","\n","  def update(self, input=None, output=None):\n","    \"\"\"Stochastic Gradient Descent update\"\"\"\n","\n","    # Probabilities of each class\n","    class_probabilities = np.exp(self.log_forward(input))\n","    batch_size, num_classes = class_probabilities.shape\n","    \n","    # Error derivative at softmax layer      \n","    I = index2onehot(output, num_classes)\n","    error = (class_probabilities - I) / batch_size\n","\n","    # Weight gradient\n","    gradient_weight = np.zeros(self.weight.shape)\n","    for l in range(batch_size):\n","      gradient_weight += np.outer(error[l, :], input[l, :])\n","      \n","    # Bias gradient\n","    gradient_bias = np.sum(error, axis=0, keepdims=True)\n","\n","    # SGD update\n","    self.weight = self.weight - self.learning_rate * gradient_weight\n","    self.bias = self.bias - self.learning_rate * gradient_bias"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DtKyzCM5OGA7"},"source":["Instantiate model and data classes. Check the initial accuracy of the model. This should be close to 50% since we are on a\n","binary prediction task and the model is not trained yet."]},{"cell_type":"code","metadata":{"id":"4phg6KlEOG-5","executionInfo":{"status":"ok","timestamp":1627408069611,"user_tz":-330,"elapsed":407,"user":{"displayName":"Siddharth Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzyE_nM4YvSHuRfgimq1KB8lKCYvlaadB3eT23qw=s64","userId":"11688753670621557151"}}},"source":["# Instantiate model\n","model = NumpyLogLinear(\n","    input_size=corpus.nr_features,\n","    num_classes=2,\n","    learning_rate=0.05\n",")\n","\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"30Kx3iOsEGUP","executionInfo":{"status":"ok","timestamp":1627408070917,"user_tz":-330,"elapsed":13,"user":{"displayName":"Siddharth Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzyE_nM4YvSHuRfgimq1KB8lKCYvlaadB3eT23qw=s64","userId":"11688753670621557151"}},"outputId":"5cf902ae-d3c6-4315-baf1-11049ee164d6"},"source":["# Define number of epochs and batch size\n","num_epochs = 10\n","batch_size = 30\n","\n","# Instantiate data iterators\n","train_batches = data.batches('train', batch_size=batch_size)\n","test_set = data.batches('test', batch_size=None)[0]\n","\n","# Check initial accuracy\n","hat_y = model.predict(input=test_set['input'])\n","accuracy = 100*np.mean(hat_y == test_set['output'])\n","print(f'Initial accuracy {accuracy}')"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Initial accuracy 51.24999999999999\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eTGIhi9eFOdH"},"source":["rain the model with simple batch stochastic gradient descent. Be sure to understand each of the steps involved, including\n","the code running inside of the model class. We will be wokring on a more complex version of the model in the upcoming\n","exercise"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gz9ltXT8FPSy","executionInfo":{"status":"ok","timestamp":1627408372611,"user_tz":-330,"elapsed":7045,"user":{"displayName":"Siddharth Singh","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjzyE_nM4YvSHuRfgimq1KB8lKCYvlaadB3eT23qw=s64","userId":"11688753670621557151"}},"outputId":"f87af3db-79d8-4b7d-a775-02f23334b57b"},"source":["# Epoch loop\n","for epoch in range(num_epochs):\n","\n","  # Batch loop\n","  for batch in train_batches:\n","    model.update(input=batch['input'], output=batch['output'])\n","\n","    # Prediction for this epoch\n","    hat_y = model.predict(input=test_set['input'])\n","\n","    # Evaluation\n","    accuracy = 100*np.mean(hat_y == test_set['output'])\n","    print(f\"Epoch {epoch+1}: accuracy {accuracy}\")"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Epoch 1: accuracy 52.75\n","Epoch 1: accuracy 52.25\n","Epoch 1: accuracy 53.25\n","Epoch 1: accuracy 51.24999999999999\n","Epoch 1: accuracy 56.00000000000001\n","Epoch 1: accuracy 57.25\n","Epoch 1: accuracy 55.25\n","Epoch 1: accuracy 56.99999999999999\n","Epoch 1: accuracy 56.75\n","Epoch 1: accuracy 57.25\n","Epoch 1: accuracy 56.99999999999999\n","Epoch 1: accuracy 57.49999999999999\n","Epoch 1: accuracy 60.0\n","Epoch 1: accuracy 59.75\n","Epoch 1: accuracy 59.25\n","Epoch 1: accuracy 59.5\n","Epoch 1: accuracy 60.5\n","Epoch 1: accuracy 62.5\n","Epoch 1: accuracy 62.0\n","Epoch 1: accuracy 63.0\n","Epoch 1: accuracy 62.74999999999999\n","Epoch 1: accuracy 63.5\n","Epoch 1: accuracy 63.0\n","Epoch 1: accuracy 63.0\n","Epoch 1: accuracy 65.0\n","Epoch 1: accuracy 66.5\n","Epoch 1: accuracy 66.25\n","Epoch 1: accuracy 60.75000000000001\n","Epoch 1: accuracy 62.74999999999999\n","Epoch 1: accuracy 67.25\n","Epoch 1: accuracy 66.5\n","Epoch 1: accuracy 64.5\n","Epoch 1: accuracy 69.0\n","Epoch 1: accuracy 67.5\n","Epoch 1: accuracy 68.0\n","Epoch 1: accuracy 68.0\n","Epoch 1: accuracy 67.0\n","Epoch 1: accuracy 68.25\n","Epoch 1: accuracy 68.25\n","Epoch 1: accuracy 67.5\n","Epoch 1: accuracy 68.75\n","Epoch 1: accuracy 67.25\n","Epoch 1: accuracy 66.5\n","Epoch 1: accuracy 68.75\n","Epoch 1: accuracy 69.75\n","Epoch 1: accuracy 69.25\n","Epoch 1: accuracy 68.75\n","Epoch 1: accuracy 69.5\n","Epoch 1: accuracy 67.25\n","Epoch 1: accuracy 70.75\n","Epoch 1: accuracy 65.25\n","Epoch 1: accuracy 69.0\n","Epoch 1: accuracy 66.75\n","Epoch 1: accuracy 60.5\n","Epoch 2: accuracy 67.75\n","Epoch 2: accuracy 67.5\n","Epoch 2: accuracy 69.0\n","Epoch 2: accuracy 67.75\n","Epoch 2: accuracy 67.5\n","Epoch 2: accuracy 69.25\n","Epoch 2: accuracy 68.75\n","Epoch 2: accuracy 69.5\n","Epoch 2: accuracy 69.5\n","Epoch 2: accuracy 69.5\n","Epoch 2: accuracy 68.5\n","Epoch 2: accuracy 69.75\n","Epoch 2: accuracy 70.0\n","Epoch 2: accuracy 71.25\n","Epoch 2: accuracy 66.0\n","Epoch 2: accuracy 71.25\n","Epoch 2: accuracy 71.25\n","Epoch 2: accuracy 73.0\n","Epoch 2: accuracy 71.25\n","Epoch 2: accuracy 71.0\n","Epoch 2: accuracy 72.0\n","Epoch 2: accuracy 72.25\n","Epoch 2: accuracy 71.5\n","Epoch 2: accuracy 71.5\n","Epoch 2: accuracy 71.75\n","Epoch 2: accuracy 73.0\n","Epoch 2: accuracy 73.25\n","Epoch 2: accuracy 71.5\n","Epoch 2: accuracy 71.0\n","Epoch 2: accuracy 72.5\n","Epoch 2: accuracy 73.25\n","Epoch 2: accuracy 73.0\n","Epoch 2: accuracy 72.75\n","Epoch 2: accuracy 73.5\n","Epoch 2: accuracy 73.0\n","Epoch 2: accuracy 73.75\n","Epoch 2: accuracy 74.0\n","Epoch 2: accuracy 74.0\n","Epoch 2: accuracy 73.0\n","Epoch 2: accuracy 75.5\n","Epoch 2: accuracy 74.0\n","Epoch 2: accuracy 72.25\n","Epoch 2: accuracy 71.75\n","Epoch 2: accuracy 75.0\n","Epoch 2: accuracy 74.75\n","Epoch 2: accuracy 74.75\n","Epoch 2: accuracy 74.75\n","Epoch 2: accuracy 74.75\n","Epoch 2: accuracy 73.0\n","Epoch 2: accuracy 73.25\n","Epoch 2: accuracy 72.75\n","Epoch 2: accuracy 74.25\n","Epoch 2: accuracy 73.25\n","Epoch 2: accuracy 74.0\n","Epoch 3: accuracy 74.0\n","Epoch 3: accuracy 74.0\n","Epoch 3: accuracy 73.75\n","Epoch 3: accuracy 74.5\n","Epoch 3: accuracy 73.75\n","Epoch 3: accuracy 74.0\n","Epoch 3: accuracy 74.5\n","Epoch 3: accuracy 75.0\n","Epoch 3: accuracy 74.25\n","Epoch 3: accuracy 74.0\n","Epoch 3: accuracy 75.25\n","Epoch 3: accuracy 74.75\n","Epoch 3: accuracy 74.0\n","Epoch 3: accuracy 76.0\n","Epoch 3: accuracy 75.25\n","Epoch 3: accuracy 73.75\n","Epoch 3: accuracy 74.0\n","Epoch 3: accuracy 73.5\n","Epoch 3: accuracy 74.25\n","Epoch 3: accuracy 74.75\n","Epoch 3: accuracy 75.0\n","Epoch 3: accuracy 74.75\n","Epoch 3: accuracy 75.0\n","Epoch 3: accuracy 74.5\n","Epoch 3: accuracy 75.0\n","Epoch 3: accuracy 74.75\n","Epoch 3: accuracy 74.75\n","Epoch 3: accuracy 75.0\n","Epoch 3: accuracy 75.0\n","Epoch 3: accuracy 74.5\n","Epoch 3: accuracy 74.25\n","Epoch 3: accuracy 74.5\n","Epoch 3: accuracy 74.75\n","Epoch 3: accuracy 74.75\n","Epoch 3: accuracy 75.25\n","Epoch 3: accuracy 75.0\n","Epoch 3: accuracy 76.0\n","Epoch 3: accuracy 74.75\n","Epoch 3: accuracy 75.0\n","Epoch 3: accuracy 75.25\n","Epoch 3: accuracy 75.25\n","Epoch 3: accuracy 75.5\n","Epoch 3: accuracy 75.75\n","Epoch 3: accuracy 75.5\n","Epoch 3: accuracy 76.0\n","Epoch 3: accuracy 76.0\n","Epoch 3: accuracy 76.5\n","Epoch 3: accuracy 76.25\n","Epoch 3: accuracy 76.0\n","Epoch 3: accuracy 75.75\n","Epoch 3: accuracy 75.75\n","Epoch 3: accuracy 76.0\n","Epoch 3: accuracy 76.25\n","Epoch 3: accuracy 76.75\n","Epoch 4: accuracy 75.5\n","Epoch 4: accuracy 77.25\n","Epoch 4: accuracy 77.75\n","Epoch 4: accuracy 77.5\n","Epoch 4: accuracy 76.25\n","Epoch 4: accuracy 74.75\n","Epoch 4: accuracy 75.5\n","Epoch 4: accuracy 75.75\n","Epoch 4: accuracy 75.25\n","Epoch 4: accuracy 75.0\n","Epoch 4: accuracy 76.0\n","Epoch 4: accuracy 76.0\n","Epoch 4: accuracy 75.75\n","Epoch 4: accuracy 76.0\n","Epoch 4: accuracy 75.0\n","Epoch 4: accuracy 75.25\n","Epoch 4: accuracy 75.25\n","Epoch 4: accuracy 75.75\n","Epoch 4: accuracy 76.0\n","Epoch 4: accuracy 76.75\n","Epoch 4: accuracy 76.75\n","Epoch 4: accuracy 76.25\n","Epoch 4: accuracy 75.75\n","Epoch 4: accuracy 75.0\n","Epoch 4: accuracy 76.0\n","Epoch 4: accuracy 77.0\n","Epoch 4: accuracy 76.5\n","Epoch 4: accuracy 76.75\n","Epoch 4: accuracy 77.0\n","Epoch 4: accuracy 75.75\n","Epoch 4: accuracy 75.25\n","Epoch 4: accuracy 77.0\n","Epoch 4: accuracy 77.25\n","Epoch 4: accuracy 76.75\n","Epoch 4: accuracy 76.5\n","Epoch 4: accuracy 77.0\n","Epoch 4: accuracy 77.25\n","Epoch 4: accuracy 77.5\n","Epoch 4: accuracy 77.0\n","Epoch 4: accuracy 77.25\n","Epoch 4: accuracy 77.25\n","Epoch 4: accuracy 77.25\n","Epoch 4: accuracy 77.0\n","Epoch 4: accuracy 76.75\n","Epoch 4: accuracy 77.25\n","Epoch 4: accuracy 77.25\n","Epoch 4: accuracy 77.75\n","Epoch 4: accuracy 77.75\n","Epoch 4: accuracy 77.25\n","Epoch 4: accuracy 77.75\n","Epoch 4: accuracy 77.25\n","Epoch 4: accuracy 77.75\n","Epoch 4: accuracy 77.75\n","Epoch 4: accuracy 77.75\n","Epoch 5: accuracy 77.5\n","Epoch 5: accuracy 78.75\n","Epoch 5: accuracy 78.75\n","Epoch 5: accuracy 78.5\n","Epoch 5: accuracy 78.25\n","Epoch 5: accuracy 76.5\n","Epoch 5: accuracy 76.75\n","Epoch 5: accuracy 79.0\n","Epoch 5: accuracy 76.75\n","Epoch 5: accuracy 76.75\n","Epoch 5: accuracy 77.0\n","Epoch 5: accuracy 77.0\n","Epoch 5: accuracy 77.0\n","Epoch 5: accuracy 76.5\n","Epoch 5: accuracy 77.0\n","Epoch 5: accuracy 76.75\n","Epoch 5: accuracy 76.75\n","Epoch 5: accuracy 76.25\n","Epoch 5: accuracy 77.0\n","Epoch 5: accuracy 78.25\n","Epoch 5: accuracy 77.5\n","Epoch 5: accuracy 78.0\n","Epoch 5: accuracy 77.75\n","Epoch 5: accuracy 76.0\n","Epoch 5: accuracy 77.25\n","Epoch 5: accuracy 77.25\n","Epoch 5: accuracy 77.25\n","Epoch 5: accuracy 78.75\n","Epoch 5: accuracy 78.25\n","Epoch 5: accuracy 77.5\n","Epoch 5: accuracy 76.25\n","Epoch 5: accuracy 77.5\n","Epoch 5: accuracy 78.25\n","Epoch 5: accuracy 78.25\n","Epoch 5: accuracy 78.5\n","Epoch 5: accuracy 78.25\n","Epoch 5: accuracy 79.0\n","Epoch 5: accuracy 79.0\n","Epoch 5: accuracy 78.75\n","Epoch 5: accuracy 78.5\n","Epoch 5: accuracy 78.0\n","Epoch 5: accuracy 78.5\n","Epoch 5: accuracy 78.75\n","Epoch 5: accuracy 77.75\n","Epoch 5: accuracy 78.5\n","Epoch 5: accuracy 78.25\n","Epoch 5: accuracy 78.75\n","Epoch 5: accuracy 78.75\n","Epoch 5: accuracy 78.25\n","Epoch 5: accuracy 78.5\n","Epoch 5: accuracy 79.25\n","Epoch 5: accuracy 79.0\n","Epoch 5: accuracy 79.0\n","Epoch 5: accuracy 78.25\n","Epoch 6: accuracy 79.75\n","Epoch 6: accuracy 79.5\n","Epoch 6: accuracy 80.0\n","Epoch 6: accuracy 80.0\n","Epoch 6: accuracy 79.5\n","Epoch 6: accuracy 78.0\n","Epoch 6: accuracy 78.25\n","Epoch 6: accuracy 80.0\n","Epoch 6: accuracy 79.25\n","Epoch 6: accuracy 78.0\n","Epoch 6: accuracy 78.75\n","Epoch 6: accuracy 78.25\n","Epoch 6: accuracy 78.25\n","Epoch 6: accuracy 78.5\n","Epoch 6: accuracy 78.5\n","Epoch 6: accuracy 78.25\n","Epoch 6: accuracy 78.25\n","Epoch 6: accuracy 78.0\n","Epoch 6: accuracy 79.0\n","Epoch 6: accuracy 79.5\n","Epoch 6: accuracy 79.25\n","Epoch 6: accuracy 79.25\n","Epoch 6: accuracy 78.75\n","Epoch 6: accuracy 78.75\n","Epoch 6: accuracy 78.75\n","Epoch 6: accuracy 78.25\n","Epoch 6: accuracy 78.75\n","Epoch 6: accuracy 79.5\n","Epoch 6: accuracy 79.0\n","Epoch 6: accuracy 79.0\n","Epoch 6: accuracy 79.25\n","Epoch 6: accuracy 79.25\n","Epoch 6: accuracy 80.25\n","Epoch 6: accuracy 79.0\n","Epoch 6: accuracy 79.25\n","Epoch 6: accuracy 80.0\n","Epoch 6: accuracy 80.25\n","Epoch 6: accuracy 79.5\n","Epoch 6: accuracy 79.75\n","Epoch 6: accuracy 79.5\n","Epoch 6: accuracy 79.25\n","Epoch 6: accuracy 79.5\n","Epoch 6: accuracy 80.0\n","Epoch 6: accuracy 79.5\n","Epoch 6: accuracy 79.25\n","Epoch 6: accuracy 80.0\n","Epoch 6: accuracy 79.75\n","Epoch 6: accuracy 80.0\n","Epoch 6: accuracy 79.75\n","Epoch 6: accuracy 79.25\n","Epoch 6: accuracy 79.5\n","Epoch 6: accuracy 80.25\n","Epoch 6: accuracy 79.5\n","Epoch 6: accuracy 79.0\n","Epoch 7: accuracy 80.5\n","Epoch 7: accuracy 80.25\n","Epoch 7: accuracy 79.25\n","Epoch 7: accuracy 80.5\n","Epoch 7: accuracy 80.5\n","Epoch 7: accuracy 80.0\n","Epoch 7: accuracy 79.5\n","Epoch 7: accuracy 81.0\n","Epoch 7: accuracy 80.0\n","Epoch 7: accuracy 78.75\n","Epoch 7: accuracy 79.75\n","Epoch 7: accuracy 79.0\n","Epoch 7: accuracy 79.0\n","Epoch 7: accuracy 79.25\n","Epoch 7: accuracy 80.0\n","Epoch 7: accuracy 79.0\n","Epoch 7: accuracy 79.0\n","Epoch 7: accuracy 79.0\n","Epoch 7: accuracy 79.5\n","Epoch 7: accuracy 80.5\n","Epoch 7: accuracy 79.75\n","Epoch 7: accuracy 79.5\n","Epoch 7: accuracy 79.5\n","Epoch 7: accuracy 79.0\n","Epoch 7: accuracy 79.5\n","Epoch 7: accuracy 79.5\n","Epoch 7: accuracy 80.0\n","Epoch 7: accuracy 80.75\n","Epoch 7: accuracy 80.25\n","Epoch 7: accuracy 79.75\n","Epoch 7: accuracy 79.0\n","Epoch 7: accuracy 80.0\n","Epoch 7: accuracy 80.75\n","Epoch 7: accuracy 80.5\n","Epoch 7: accuracy 80.5\n","Epoch 7: accuracy 80.25\n","Epoch 7: accuracy 80.5\n","Epoch 7: accuracy 80.25\n","Epoch 7: accuracy 80.5\n","Epoch 7: accuracy 80.0\n","Epoch 7: accuracy 80.25\n","Epoch 7: accuracy 80.25\n","Epoch 7: accuracy 80.5\n","Epoch 7: accuracy 80.75\n","Epoch 7: accuracy 80.25\n","Epoch 7: accuracy 80.5\n","Epoch 7: accuracy 80.75\n","Epoch 7: accuracy 80.75\n","Epoch 7: accuracy 81.5\n","Epoch 7: accuracy 80.25\n","Epoch 7: accuracy 80.0\n","Epoch 7: accuracy 80.75\n","Epoch 7: accuracy 79.75\n","Epoch 7: accuracy 79.75\n","Epoch 8: accuracy 81.25\n","Epoch 8: accuracy 80.75\n","Epoch 8: accuracy 80.75\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 80.75\n","Epoch 8: accuracy 80.75\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 80.75\n","Epoch 8: accuracy 80.25\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 80.25\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 80.0\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 80.0\n","Epoch 8: accuracy 80.25\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 80.0\n","Epoch 8: accuracy 80.75\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 81.25\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 80.25\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 80.5\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 81.25\n","Epoch 8: accuracy 81.25\n","Epoch 8: accuracy 81.25\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 80.75\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 80.75\n","Epoch 8: accuracy 81.25\n","Epoch 8: accuracy 80.75\n","Epoch 8: accuracy 81.25\n","Epoch 8: accuracy 81.75\n","Epoch 8: accuracy 82.0\n","Epoch 8: accuracy 81.75\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 81.0\n","Epoch 8: accuracy 80.75\n","Epoch 8: accuracy 79.75\n","Epoch 9: accuracy 81.25\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 80.75\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.75\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.25\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.75\n","Epoch 9: accuracy 81.75\n","Epoch 9: accuracy 81.25\n","Epoch 9: accuracy 81.75\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.25\n","Epoch 9: accuracy 80.5\n","Epoch 9: accuracy 81.25\n","Epoch 9: accuracy 81.25\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.75\n","Epoch 9: accuracy 81.25\n","Epoch 9: accuracy 80.5\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.25\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.25\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.25\n","Epoch 9: accuracy 80.75\n","Epoch 9: accuracy 81.25\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 81.5\n","Epoch 9: accuracy 82.0\n","Epoch 9: accuracy 81.75\n","Epoch 9: accuracy 82.25\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.0\n","Epoch 9: accuracy 81.25\n","Epoch 10: accuracy 81.25\n","Epoch 10: accuracy 81.25\n","Epoch 10: accuracy 81.0\n","Epoch 10: accuracy 81.25\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 82.0\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.25\n","Epoch 10: accuracy 82.25\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 82.0\n","Epoch 10: accuracy 82.0\n","Epoch 10: accuracy 82.0\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 82.0\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 82.0\n","Epoch 10: accuracy 82.0\n","Epoch 10: accuracy 82.25\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.25\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.25\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.25\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.25\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.0\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 82.5\n","Epoch 10: accuracy 82.25\n","Epoch 10: accuracy 81.75\n","Epoch 10: accuracy 81.25\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.5\n","Epoch 10: accuracy 81.75\n"],"name":"stdout"}]}]}